{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "import numpy as np\n",
    "# import keys\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AI GPT API TEST\n",
    "# REQUIRES PAYMENT :(\n",
    "# import os\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = keys.OPEN_AI_KEY\n",
    "\n",
    "# response = openai.Completion.create(\n",
    "#   model=\"gpt-3.5-turbo\",\n",
    "#   messages=[\n",
    "#     {\n",
    "#       \"role\": \"user\",\n",
    "#       \"content\": \"I will provide you with a few paragraphs from news articles. Identify the subject (usually a company) and the sentiment. If possible, instead of writing the company name, return the company symbol instead (for example, instead of Apple, return APPL; instead of Alphabet, return GOOG). Sentiment should be a number from -1.0 (indicating a very negative sentiment) and 1.0 (indicating a very positive sentiment). The response format should look like this:\\n{'APPL', -0.3}\\n\\nif there are multiple companies with identifiable sentiments in the article, return multiple lines. For example:\\n{'APPL', -0.3}\\n{'GOOG', 0.7}\\n\\n Do not provide further information.\"\n",
    "#     }\n",
    "#   ],\n",
    "#   temperature=1,\n",
    "#   max_tokens=256,\n",
    "#   top_p=1,\n",
    "#   frequency_penalty=0,\n",
    "#   presence_penalty=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_symbols(base_dir=\"data\"):\n",
    "    \"\"\"Return a list of symbols from a directory by removing the file extension `csv`.\"\"\"\n",
    "    file_list = os.listdir(base_dir)\n",
    "    res =[]\n",
    "\n",
    "    for file in file_list:\n",
    "        file = file.split('.')\n",
    "        if file[1] ==\"csv\":\n",
    "            res.append(file[0])\n",
    "    return res\n",
    "        \n",
    "def symbol_to_path(symbol, base_dir= \"data\"):\n",
    "    \"\"\"Return CSV file path given ticker symbol.\"\"\"\n",
    "    return os.path.join(base_dir, \"{}.csv\".format(str(symbol)))\n",
    "\n",
    "def get_data(symbols, dates):\n",
    "    \"\"\"Read stock data (adjusted close) for given symbols from CSV files.\"\"\"\n",
    "    df_container = pd.DataFrame(index=dates)\n",
    "    if 'SPY' not in symbols:  # add SPY for reference, if absent\n",
    "        symbols.insert(0, 'SPY')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    if len(symbols) > 1:\n",
    "        for symbol in symbols:\n",
    "            if symbol == \"SPY\":\n",
    "                df_temp = pd.read_csv(\"data/\" + \"SPY\" + \".csv\", index_col= \"Date\", parse_dates= True, usecols=['Date', 'Adj Close'], na_values=\"nan\")\n",
    "                df_temp = df_temp.rename(columns={'Adj Close':\"SPY\"})\n",
    "                df_container = df_container.join(df_temp)\n",
    "                df_container = df_container.dropna();\n",
    "            else:\n",
    "            \n",
    "                df_temp = pd.read_csv(\"data/\" + symbol + \".csv\", index_col= \"Date\", parse_dates= True, usecols=['Date', 'Adj Close'], na_values=\"nan\")\n",
    "                df_temp = df_temp.rename(columns={'Adj Close':symbol})\n",
    "                df_container = df_container.join(df_temp)\n",
    "    else:\n",
    "        df_temp = pd.read_csv(\"data/\" + symbols[0] + \".csv\", index_col= \"Date\", parse_dates= True, usecols=['Date', 'Adj Close'], na_values=\"nan\")\n",
    "        df_temp = df_temp.rename(columns={'Adj Close':symbols[0]})\n",
    "        df_container = df_container.join(df_temp)\n",
    "\n",
    "    return df_container\n",
    "\n",
    "def normalise_data(df:pd.DataFrame, frame_of_reference= 0):\n",
    "    \"\"\"Normalises the data based on the frame of reference\"\"\"\n",
    "    return df/df.iloc[frame_of_reference]\n",
    "\n",
    "def get_rolling_mean(df:pd.DataFrame, window = 20):\n",
    "    return df.rolling(window).mean()\n",
    "\n",
    "def get_rolling_std(df:pd.DataFrame, window = 20):\n",
    "    return df.rolling(window).std()\n",
    "\n",
    "def get_bollinger_bands(df:pd.DataFrame, window = 20, num_std = 2):\n",
    "    \"\"\" Returns a tuple of Bollinger BandsÂ® `(upper band, rolling mean , lower band)`\"\"\"\n",
    "\n",
    "    rolling_mean = get_rolling_mean(df, window)\n",
    "    std = get_rolling_std(df, window)\n",
    "    upper_band = rolling_mean + num_std * std\n",
    "    lower_band = rolling_mean - num_std * std\n",
    "\n",
    "    return (upper_band, rolling_mean, lower_band)\n",
    "\n",
    "def get_daily_returns(df:pd.DataFrame):\n",
    "    df_lag = df.shift(1)\n",
    "    df_res = ((df/df_lag) - 1) * 100\n",
    "\n",
    "    df_res = df_res.fillna(0)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    df.ffill(axis=0, inplace=True)\n",
    "    df.bfill(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "def plot_dfs(dfs:list):\n",
    "    ax = dfs[0].plot()\n",
    "    for df in dfs:\n",
    "        df.plot(ax =ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressors/Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_with_input_data(df, window, y,col_name):\n",
    "    last_date = df.index[-1]\n",
    "    new_end_date = last_date + pd.DateOffset(days=window)\n",
    "    new_date_range = pd.date_range(start=last_date, end=new_end_date, freq='D')\n",
    "\n",
    "\n",
    "\n",
    "    fin = df.tail(window)\n",
    "    fin = pd.concat([fin, pd.DataFrame(index=new_date_range)])\n",
    "\n",
    "    fin.insert(0, col_name, y)\n",
    "    return fin\n",
    "\n",
    "# linear\n",
    "def linear_regression(df:pd.DataFrame, window=100):\n",
    "    \"\"\"\n",
    "    `df`dataframe to create regression from \\n\n",
    "    `window` the number of days from the dataframe to consider in the regression\n",
    "    \n",
    "    \"\"\"\n",
    "    res = df.tail(window)\n",
    "    res = res.reset_index(drop=True, inplace=False)\n",
    "    res.insert(0, 'index',res.index)\n",
    "    temp_array = res.to_numpy()\n",
    "    temp_array = np.transpose(temp_array) \n",
    "    \n",
    "    reg = lm.LinearRegression()\n",
    "    \n",
    "    reg.fit(temp_array[0].reshape(-1, 1) ,temp_array[1])\n",
    "\n",
    "    x = np.linspace(window, 2*window, window)\n",
    "    b = reg.intercept_\n",
    "    m = reg.coef_\n",
    "    y = m * x + b\n",
    "\n",
    "    # print(temp_array[0][0])\n",
    "    # print(y)\n",
    "    # print(m)\n",
    "    # print(x)\n",
    "    print(\"intercept = {}\".format(b))\n",
    "\n",
    "    # fin = overlay_with_input_data(df, window, y, \"linear reg\")\n",
    "    # fin.pop(\"SPY\")\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "# poly\n",
    "def polynomial_regression(df:pd.DataFrame, window=100, degree=3):\n",
    "    \n",
    "    # convert data to scikit-learn friendly format\n",
    "    res = df.tail(window)\n",
    "    res = res.reset_index(drop=True, inplace=False)\n",
    "    res.insert(0, 'index',res.index)\n",
    "    temp_array = res.to_numpy()\n",
    "    temp_array = np.transpose(temp_array) \n",
    "\n",
    "    x_train = temp_array[0]\n",
    "    y_train = temp_array[1]\n",
    "\n",
    "    # create poly features matrix\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    x_poly = poly_features.fit_transform(x_train.reshape(-1, 1))\n",
    "\n",
    "    # fit poly regression model\n",
    "    model = lm.LinearRegression()\n",
    "    model.fit(x_poly, y_train)\n",
    "\n",
    "\n",
    "    # use results / create predictions\n",
    "    # x_test = np.linspace(1, window, window)\n",
    "    x_test = np.linspace(window, 2*window, window)\n",
    "    x_test_poly = poly_features.transform(x_test.reshape(-1, 1))\n",
    "    y = model.predict(x_test_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # move results to dataframe\n",
    "    # fin = overlay_with_input_data(df, window, y, \"poly reg\")\n",
    "\n",
    "    # fin.pop(\"SPY\")\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "# ARIMA    \n",
    "def arima_pred(df:pd.DataFrame, p=10,d=2,q=5, days_to_predict = 30):\n",
    "\n",
    "    \n",
    "    # Step 3: Decompose the Time Series (optional)\n",
    "    # decomposition = seasonal_decompose(df['SPY'], model='additive')\n",
    "    # trend = decomposition.trend\n",
    "    # seasonal = decomposition.seasonal\n",
    "    # residual = decomposition.resid\n",
    "\n",
    "    # Step 4: Stationarize the Data (if needed)\n",
    "\n",
    "    # Step 5: Choose a Forecasting Model (e.g., ARIMA)\n",
    "    model = ARIMA(df['SPY'], order=(p, d, q))  # Replace p, d, q with appropriate values\n",
    "\n",
    "    # Step 6: Train the Model\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Step 7: Make Forecasts\n",
    "    forecast_periods = days_to_predict  # Number of periods to forecast\n",
    "    forecast = model_fit.forecast(steps=forecast_periods)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Step 8: Visualize Forecasts\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(df.index, df['SPY'], label='Original Data')\n",
    "    # plt.plot(pd.date_range(start=df.index[-1], periods=forecast_periods+1), [df['SPY'].iloc[-1]] + list(forecast), label='Forecast', color='red')\n",
    "    # plt.title('Time Series Forecast')\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Value')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # move results to dataframe\n",
    "    # fin = overlay_with_input_data(df,days_to_predict, forecast, \"ARIMA\")\n",
    "\n",
    "    # fin=fin.join(df_pred)\n",
    "\n",
    "    return forecast\n",
    "\n",
    "def sentiment_prediction(df:pd.DataFrame, input_text=\"I fucking hate my life!\", days_to_predict=30):\n",
    "    blob = TextBlob(input_text)\n",
    "\n",
    "    cumulative_sentiment = 0\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        cumulative_sentiment += sentence.sentiment.polarity\n",
    "    \n",
    "    print(\"sentiment prediction for '{input_text}' is {cumulative_sentiment}\".format(input_text=input_text,cumulative_sentiment=cumulative_sentiment ))\n",
    "\n",
    "    # make linear interp using sentiment as slope\n",
    "    x = np.linspace(1,days_to_predict,days_to_predict)\n",
    "    b = df.tail(1).values[0]\n",
    "    m = cumulative_sentiment\n",
    "    y = m * x + b\n",
    "\n",
    "    \n",
    "    # fin = overlay_with_input_data(df,days_to_predict, y, \"sentiment\")\n",
    "\n",
    "    # fin.pop(\"SPY\")\n",
    "\n",
    "    return y\n",
    "\n",
    "def generate_data(df, prediction_time, periods):\n",
    "    results = []\n",
    "\n",
    "    # ARIMA pred\n",
    "    for period in periods:\n",
    "        df_p = arima_pred(df,p=period,d=1,q=2,days_to_predict=prediction_time)\n",
    "        results.append(df_p)\n",
    "        \n",
    "    # linear pred\n",
    "    linear_reg = linear_regression(df, prediction_time)\n",
    "    # poly pred\n",
    "    polynomial_reg = polynomial_regression(df, prediction_time,2)\n",
    "    # sentiment pred\n",
    "    sentiment_reg = sentiment_prediction(df,days_to_predict=prediction_time, input_text=\"medium\")\n",
    "    # mix results\n",
    "    results.append(linear_reg)\n",
    "    results.append(polynomial_reg)\n",
    "    results.append(sentiment_reg)\n",
    "\n",
    "    return results\n",
    "\n",
    "def combine_results(df, results, weights=[1,1,2,5,2,2,3]):\n",
    "    if len(results) != len(weights):\n",
    "        raise ValueError(\"length of results and weights  is not equal: results {res} != weights {wei}.\".format(res=len(results), wei=len(weights)))\n",
    "    \n",
    "    normalisation_factor = \t1/sum(weights)\n",
    "\n",
    "    df_ensemble = pd.DataFrame(index=results[0].index)\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        df_ensemble.insert(len(df_ensemble.columns),\"result {}\".format(i),results[i])\n",
    "\n",
    "    df_ensemble = df_ensemble.multiply(weights,1)\n",
    "    df_ensemble = df_ensemble.multiply(normalisation_factor)\n",
    "    df_ensemble.insert(len(df_ensemble.columns),\"avg\",df_ensemble.sum(1))\n",
    "    df_ensemble = df_ensemble.pop(\"avg\")\n",
    "\n",
    "    # print(df_ensemble)\n",
    "\n",
    "    return df_ensemble\n",
    "\n",
    "def monte_carlo_sim(df:pd.DataFrame, num_sims=100, days_to_predict = 60):\n",
    "    df_simulation = pd.DataFrame()\n",
    "    returns = df.pct_change()\n",
    "\n",
    "    last_price = df.values[-1]\n",
    "\n",
    "    for sim in range(num_sims):\n",
    "        count = 0\n",
    "        daily_volatility = returns.std()\n",
    "\n",
    "        price_series = []\n",
    "\n",
    "        price = last_price * (1 + np.random.normal(0, daily_volatility))\n",
    "        price_series.append(price)\n",
    "\n",
    "        for day in range(days_to_predict):\n",
    "            \n",
    "            price = price_series[count] * (1+ np.random.normal(0,daily_volatility))\n",
    "            price_series.append(price)\n",
    "            count+=1\n",
    "            # print(price_series)\n",
    "            \n",
    "        \n",
    "        df_simulation[sim] = price_series\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # plt.plot(df_simulation)\n",
    "\n",
    "    # move results to dataframe\n",
    "    last_date = df.index[-1]\n",
    "    new_end_date = last_date + pd.DateOffset(days=days_to_predict)\n",
    "    new_date_range = pd.date_range(start=last_date, end=new_end_date, freq='D')\n",
    "\n",
    "\n",
    "    df_simulation.index = new_date_range\n",
    "    # df_simulation = pd.concat([s.reset_index(drop=True) for s in df_simulation.iloc[:, 0]], axis=1)\n",
    "    df_simulation = df_simulation.applymap(lambda x: x[0])\n",
    "\n",
    "    df_simulation_upper = df_simulation.max(axis=1)\n",
    "    df_simulation_lower = df_simulation.min(axis=1)\n",
    "    \n",
    "    return df_simulation,df_simulation_upper, df_simulation_lower\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\miniconda3\\envs\\stock_pred\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "def experiment():\n",
    "    # Define a date range\n",
    "    dates = pd.date_range('2010-03-01', '2010-10-29')\n",
    "    # dates = pd.date_range('2010-01-01', '2010-7-29')\n",
    "    # dates = pd.date_range('2010-01-01', '2010-12-30')\n",
    "    # dates = pd.date_range('2006-01-01', '2013-12-30')\n",
    "\n",
    "    # Choose stock symbols to read\n",
    "    symbols = path_to_symbols()\n",
    "    symbols = [symbols[4]]\n",
    "\n",
    "    days_to_forecast = 30\n",
    "\n",
    "    periods = [1 , 7 , 30 , 90]\n",
    "    # periods = [1 , 1 , 1 , 1]\n",
    "    \n",
    "    # data collection / clean-up\n",
    "    df = get_data(symbols, dates)\n",
    "\n",
    "\n",
    "    fill_missing_values(df)\n",
    "    # ax = df.plot(legend=False)\n",
    "\n",
    "\n",
    "    # df.plot()\n",
    "\n",
    "\n",
    "\n",
    "    # df_ens = ensemble(df, symbols=symbols, arima_periods=periods, prediction_time=days_to_forecast)\n",
    "    res = generate_data(df,days_to_forecast,periods=periods)\n",
    "    df_ens = combine_results(df, res)\n",
    "    \n",
    "\n",
    "    # df_mc,df_mc_u,df_mc_l = monte_carlo_sim(df,days_to_predict=days_to_forecast)\n",
    "    # # df_mc.plot(ax =ax,legend=False)\n",
    "    # df_mc_u.plot(ax=ax,legend=False)\n",
    "    # df_mc_l.plot(ax=ax,legend=False)\n",
    "    # df_ens.plot(ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4f66d7ae1f5aef923b281c78a9068b43aa8b173e58598fc900391b5d8149f17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
